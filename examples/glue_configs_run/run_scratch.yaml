# run GLUE from scratch using BERT-base config

seed: 42
output_dir: /mnt/data1/made_workspace/newlm/downstream-from-0
tokenizer:
  pretrained: "bert-base-uncased"
  max_len: 512
lm:
  # pretrained: "bert-base-uncased"
  model: # model config if you run from scratch
    config: {} # use all default BertConfig
glue:
  from_scratch: true
  tasks: ["qqp", "stsb", "wnli", "mnli-mm"]
  hf_trainer:
    args:
      evaluation_strategy: "epoch"
      save_strategy: "epoch"
      learning_rate: 0.00002
      per_device_train_batch_size: 16
      per_device_eval_batch_size: 16
      num_train_epochs: 3
      # max_steps: 50
      weight_decay: 0.01
      save_total_limit: 1
wandb:
  run_basename: exp-from-0
