seed: 42
output_dir: outputs/trial1/
tokenizer:
  pretrained: "bert-base-cased"
lm:
  pretrained: "bert-base-cased"
  # model: # model config if you run from scratch
  #   config:
  #     hidden_size: 768
  #     num_attention_heads: 12
  #     num_hidden_layers: 12
  #     intermediate_size: 3072
  #     max_position_embeddings: 1024
glue:
  # from_scratch: true
  tasks: ["cola"]
  hf_trainer:
    args:
      evaluation_strategy: "epoch"
      save_strategy: "epoch"
      learning_rate: 0.00002
      per_device_train_batch_size: 16
      per_device_eval_batch_size: 16
      num_train_epochs: 3
      weight_decay: 0.01
  # cola:
  #   hf_trainer:
  #     args:
  #       learning_rate: 0.1
wandb:
  run_basename: exp
