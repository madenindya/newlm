# seed: 10
# output_dir: /mnt/data4/made_workspace/newlm-output/bert-mixmatch/causal-to-bert-epoch-6
# tokenizer:
#   pretrained: /mnt/data4/made_workspace/newlm-output/bert-causal-en.100-percent/checkpoint-1000000
# lm:
#   model_type: bert
#   pretrained: /mnt/data4/made_workspace/newlm-output/bert-causal-en.100-percent/checkpoint-1000000
# glue:
#   tasks: ["cola", "mrpc", "rte", "stsb"] #, "qnli"] # if not defined, would train all tasks
#   hf_trainer:
#     total_batch_size: 32
#     args:
#       evaluation_strategy: "epoch"
#       save_strategy: "epoch"
#       learning_rate: 0.00002
#       per_device_train_batch_size: 16
#       per_device_eval_batch_size: 16
#       num_train_epochs: 6
#       weight_decay: 0.01
#       save_total_limit: 1
#   qnli:
#     hf_trainer:
#       args:
#         per_device_train_batch_size: 8
#         gradient_accumulation_steps: 2
# wandb:
#   run_basename: causal-to-bert-epoch-6



##########


seed: 10
output_dir: /mnt/data4/made_workspace/newlm-output/bert-mixmatch/bert-to-causal-v2-epoch-6
tokenizer:
  pretrained: /mnt/data1/made_workspace/newlm-gcp-bert-100_percent/checkpoint-1000000
lm:
  model_type: bert-causal
  pretrained: /mnt/data1/made_workspace/newlm-gcp-bert-100_percent/checkpoint-1000000
glue:
  tasks: ["cola", "mrpc", "rte", "stsb"]
  hf_trainer:
    total_batch_size: 32
    args:
      evaluation_strategy: "epoch"
      save_strategy: "epoch"
      learning_rate: 0.00002
      per_device_train_batch_size: 16
      per_device_eval_batch_size: 16
      num_train_epochs: 6
      weight_decay: 0.01
      save_total_limit: 1
  qnli:
    hf_trainer:
      args:
        per_device_train_batch_size: 8
        gradient_accumulation_steps: 2
wandb:
  run_basename: bert-to-causal-epoch-6
