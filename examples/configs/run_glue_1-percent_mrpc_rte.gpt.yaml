seed: 42
output_dir: outputs/en.1-percent-mrpc-rte.gpt.default/
tokenizer:
  pretrained: "/mnt/data3/haryoaw_workspace/projects/2021/2021_2/new-lm/newlm/outputs/en.1-percent.gpt.default/model"
  max_len: 1024
lm:
  model_type: "gpt2"
  pretrained: "outputs/en.1-percent.gpt.default/model"
glue:
  tasks: ["mrpc", "rte"]
  hf_trainer:
    args:
      evaluation_strategy: "epoch"
      save_strategy: "epoch"
      learning_rate: 0.00002
      per_device_train_batch_size: 16
      gradient_accumulation_steps: 1
      per_device_eval_batch_size: 16
      num_train_epochs: 3
      weight_decay: 0.01
  # cola:
  #   hf_trainer:
  #     args:
  #       learning_rate: 0.1
wandb:
  run_basename: gpt2-glue-0.1

