seed: 42
output_dir: outputs/trial1/
tokenizer:
  # Every params for BertWordPieceTokenizer.train
  config:
    vocab_size: 30000
    min_frequency: 2
  input_dir: examples/data/text/en.5-percent/
  max_len: 512
lm:
  model:
    # Every params for transformers.BertConfig
    config:
      vocab_size: 30000
      hidden_size: 512
      num_attention_heads: 2
      num_hidden_layers: 4
      intermediate_size: 1024
  hf_trainer:
    # Every params for transformers.TrainingArguments
    args:
      # per_gpu_train_batch_size: 64
      # must be 256 per-batch. 
      # i.e. num_GPU * accum_step * batch-size = 256
      gradient_accumulation_steps: 16
      per_device_train_batch_size: 16
      num_train_epochs: 2
      save_steps: 1000
      save_total_limit: 2
      prediction_loss_only: true
  train_path: examples/data/text/en.5-percent/text.txt
glue:
  pretrained_model: "bert-base-uncased"
  pretrained_tokenizer: "bert-base-uncased"
  # tasks: ["cola", "mnli", "qnli"] # if not defined, would train all tasks
  hf_trainer:
    args:
      evaluation_strategy: "epoch"
      save_strategy: "epoch"
      learning_rate: 0.00002
      per_device_train_batch_size: 16
      per_device_eval_batch_size: 16
      num_train_epochs: 2
      weight_decay: 0.01
  max_len: 128
  # cola: # custom training arguments for each task
  #   hf_trainer:
  #     args:
  #       learning_rate: 0.1
wandb:
  run_basename: exp1
