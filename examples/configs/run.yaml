tokenizer:
  config:
    vocab_size: 30000
    min_frequency: 2
  input_dir: examples/data/text/id_csui/
  output_dir: outputs/trial1/
lm:
  model:
    config:
      vocab_size: 30000
      hidden_size: 512
      num_attention_heads: 2
      num_hidden_layers: 4
      intermediate_size: 1024
  hf_trainer:
    args:
      # per_gpu_train_batch_size: 64
      overwrite_output_dir: true
      num_train_epochs: 2
      save_steps: 10000
      save_total_limit: 2
      prediction_loss_only: true
  train_path: examples/data/text/id_csui/text.txt
  output_dir: outputs/trial1/
