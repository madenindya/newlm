seed: 10
output_dir: /mnt/data1/made_workspace/newlm-output/elmo.1-percent.-l2r-r2l-ensemble-trial/r2l
# output_dir: /mnt/data1/made_workspace/newlm-output/elmo.1-percent.-l2r-r2l-ensemble-trial/l2r
tokenizer:
  pretrained: /mnt/data1/made_workspace/newlm-output/bert-causal-en.1-percent-r2l/model
  # pretrained: /mnt/data4/made_workspace/newlm-output/bert-causal-en.1-percent-rerun/model/
lm:
  model_type: "bert-causal-r2l" # train R2L
  pretrained: /mnt/data1/made_workspace/newlm-output/bert-causal-en.1-percent-r2l/model
  # model_type: "bert-causal" # train L2R
  # pretrained: /mnt/data4/made_workspace/newlm-output/bert-causal-en.1-percent-rerun/model/
glue:
  tasks: ["rte", "stsb", "sst2"] # if not defined, would train all tasks
  hf_trainer:
    total_batch_size: 32
    args:
      evaluation_strategy: "epoch"
      save_strategy: "epoch"
      learning_rate: 0.00002
      per_device_train_batch_size: 16
      per_device_eval_batch_size: 16
      num_train_epochs: 3
      weight_decay: 0.01
      save_total_limit: 1
  qnli:
    hf_trainer:
      args:
        per_device_train_batch_size: 8
        gradient_accumulation_steps: 2
wandb:
  run_basename: bert-causal-r2l-rerun
  # run_basename: bert-causal-l2r-rerun
