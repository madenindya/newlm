seed: 42
output_dir: /mnt/data1/made_workspace/newlm1
tokenizer:
  pretrained: /mnt/data1/made_workspace/newlm1/model
lm:
  pretrained: /mnt/data1/made_workspace/newlm1/model
glue:
  tasks: ["mnli-mm", "qnli", "rte"]
  hf_trainer:
    args:
      evaluation_strategy: "epoch"
      save_strategy: "epoch"
      learning_rate: 0.00002
      per_device_train_batch_size: 16
      per_device_eval_batch_size: 16
      num_train_epochs: 3
      weight_decay: 0.01
      save_total_limit: 2
  qnli:
    hf_trainer:
      args:
        per_device_train_batch_size: 8
        gradient_accumulation_steps: 2
wandb:
  run_basename: exp-all-md
