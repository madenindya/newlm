{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data3/haryoaw_workspace/projects/2021/2021_2/new-lm/newlm\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\r\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "!echo $CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newlm.lm.elmo.modeling_elmo.elmo_model import ELMOGPTHeadModel\r\n",
    "from newlm.lm.elmo.modeling_elmo.elmo_config import ELMOConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(vocab_size=55000, pad_token_id=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_model = ELMOGPTHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer.batch_encode_plus(['my name is haryo', 'i am a student'], return_tensors='pt', padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_model(**batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try new LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    PreTrainedTokenizer,\n",
    "    TextDataset,\n",
    "    LineByLineTextDataset,\n",
    "    TextDatasetForNextSentencePrediction,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from newlm.utils.file_util import create_dir\n",
    "import wandb\n",
    "from loguru import logger\n",
    "from newlm.lm.elmo.modeling_elmo.elmo_model import ELMOGPTHeadModel\n",
    "from newlm.lm.elmo.modeling_elmo.elmo_config import ELMOConfig\n",
    "from transformers import GPT2Config\n",
    "# TODO:\n",
    "# - take out data from this class then pass it only on training\n",
    "\n",
    "\n",
    "class ELMOLMBuilder:\n",
    "    \"\"\"\n",
    "    Wrapper class to train BERT LM. Here, we utilize HuggingFace Trainer to train the model.\n",
    "    You only need to define your tokenizer and training data, then it would train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_config,\n",
    "        tokenizer: Union[str, PreTrainedTokenizer],\n",
    "        max_len: int = 512,\n",
    "    ):\n",
    "        self.max_len = max_len\n",
    "        self.model_config = model_config\n",
    "        self.tokenizer = tokenizer\n",
    "        if type(tokenizer) == str:\n",
    "            self.tokenizer = BertTokenizerFast.from_pretrained(\n",
    "                tokenizer,\n",
    "                max_len=self.max_len,\n",
    "                do_lower_case=False,  # uncased\n",
    "            )\n",
    "\n",
    "        self.data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False,\n",
    "            mlm_probability=0.15,\n",
    "        )\n",
    "\n",
    "    def create(\n",
    "        self,\n",
    "        train_path: str,\n",
    "        output_dir: str,\n",
    "        training_args: dict,\n",
    "        use_nsp: bool = False,\n",
    "        train_params={},\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train BERT MLM (and NSP (optional)) from scratch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_path : str\n",
    "            Path to training file\n",
    "        output_dir : str\n",
    "            Path to output dir\n",
    "        training_args : dict\n",
    "            Training params based on transformers.TrainingArguments\n",
    "        use_nsp : bool\n",
    "            Wether to train NSP too or not, default: True\n",
    "        \"\"\"\n",
    "        config = GPT2Config(**self.model_config)\n",
    "        dataset = self.__get_dataset(train_path)\n",
    "        model = ELMOGPTHeadModel(config=config)\n",
    "\n",
    "        create_dir(output_dir)\n",
    "        args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            overwrite_output_dir=True,\n",
    "            **training_args,\n",
    "        )\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            train_dataset=dataset,\n",
    "            data_collator=self.data_collator,\n",
    "        )\n",
    "\n",
    "        self.__resolve_checkpoint(train_params, output_dir)\n",
    "        if \"resume_from_checkpoint\" in train_params:\n",
    "            logger.info(\n",
    "                f\"Resume training from checkpoint {train_params['resume_from_checkpoint']}\"\n",
    "            )\n",
    "        trainer.train(**train_params)\n",
    "        trainer.save_model(output_dir)\n",
    "\n",
    "        wandb.finish()\n",
    "\n",
    "    def __get_dataset_via_ds(self, train_path):\n",
    "        dataset = load_dataset(\"text\", data_files=train_path)\n",
    "\n",
    "        def preprocess_function(examples):\n",
    "            return self.tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "        encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "        return encoded_dataset[\"train\"]\n",
    "\n",
    "    def __get_dataset(self, train_path):\n",
    "        dataset = self.__get_dataset_via_ds(train_path)[\"input_ids\"]\n",
    "        print(len(dataset))\n",
    "\n",
    "        logger.info(\"Constructing roBERTa style dataset\")\n",
    "        # merge multiple lines to form a single example\n",
    "        merged_dataset = []\n",
    "        \n",
    "        # init the tmp with the first dataset\n",
    "        tmp = dataset[0]\n",
    "\n",
    "        for d in tqdm(dataset[1:]):\n",
    "            # special case, empty line that indicates document breaks\n",
    "            # i.e. [CLS] [SEP]\n",
    "            # in this case, we want to keep the [SEP]\n",
    "            if len(d) == 2:\n",
    "                d.append(d[-1]) # convert to [CLS] [SEP] [SEP]\n",
    "            \n",
    "            d_len = len(d) - 2  # exclude the first [CLS] and last [SEP]\n",
    "\n",
    "            if len(tmp) + d_len < self.max_len:\n",
    "                # tmp = [CLS] xxx yyy zzz [SEP]\n",
    "                # d = [CLS] aaa bbb [SEP]\n",
    "                # resulting tmp = [CLS] xxx yyy zzz aaa bbb [SEP]\n",
    "\n",
    "                # for a special case of d = [CLS] [SEP] [SEP]\n",
    "                # resulting tmp will be:\n",
    "                # [CLS] xxx yyy zzz [SEP] [SEP]\n",
    "                # which later be added with the next sentence to form:\n",
    "                # [CLS] xxx yyy zzz [SEP] ooo ppp [SEP]\n",
    "                tmp = tmp[:-1] + d[1:]\n",
    "            else:\n",
    "                merged_dataset.append(tmp)\n",
    "                tmp = d\n",
    "        \n",
    "        # add the leftover tmp\n",
    "        merged_dataset.append(tmp)\n",
    "\n",
    "        merged_dataset = [{\"input_ids\": d} for d in merged_dataset]\n",
    "        \n",
    "        return merged_dataset\n",
    "\n",
    "    def __resolve_checkpoint(self, train_params, output_dir):\n",
    "        resume_from = train_params.get(\"resume_from_checkpoint\")\n",
    "        if resume_from == \"latest\":\n",
    "            latest_ckpt = \"\"\n",
    "            max_ckpt = 0\n",
    "            for d in os.listdir(output_dir):\n",
    "                if \"checkpoint\" in d:\n",
    "                    ckpt = int(d.split(\"checkpoint-\")[1])\n",
    "                    if ckpt > max_ckpt:\n",
    "                        max_ckpt = ckpt\n",
    "                        latest_ckpt = str(Path(output_dir) / d)\n",
    "            train_params[\"resume_from_checkpoint\"] = (\n",
    "                latest_ckpt if max_ckpt > 0 else output_dir\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = dict(pad_token_id=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_builder = ELMOLMBuilder(model_config, tokenizer, max_len=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_trainer_args = {\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "      \"num_train_epochs\": 1,\n",
    "      \"save_steps\": 500,\n",
    "      \"save_total_limit\": 2,\n",
    "      \"prediction_loss_only\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6e4518841184bfd3\n",
      "Reusing dataset text (/home/haryoaw/.cache/huggingface/datasets/text/default-6e4518841184bfd3/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
      "Parameter 'function'=<function ELMOLMBuilder.__get_dataset_via_ds.<locals>.preprocess_function at 0x7f73e6694280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f59cd424fe455fb52549995ac54cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-06 11:36:17.753 | INFO     | __main__:__get_dataset:119 - Constructing roBERTa style dataset\n",
      "100%|██████████| 62/62 [00:00<00:00, 305577.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkata-research\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.23<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">../coba</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/kata-research/huggingface\" target=\"_blank\">https://wandb.ai/kata-research/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/kata-research/huggingface/runs/1tj744hf\" target=\"_blank\">https://wandb.ai/kata-research/huggingface/runs/1tj744hf</a><br/>\n",
       "                Run data is saved locally in <code>/mnt/data3/haryoaw_workspace/projects/2021/2021_2/new-lm/newlm/wandb/run-20210906_113629-1tj744hf</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 1451791<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.03MB of 0.03MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/mnt/data3/haryoaw_workspace/projects/2021/2021_2/new-lm/newlm/wandb/run-20210906_113629-1tj744hf/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/mnt/data3/haryoaw_workspace/projects/2021/2021_2/new-lm/newlm/wandb/run-20210906_113629-1tj744hf/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/train_runtime</td><td>6.4636</td></tr><tr><td>train/train_samples_per_second</td><td>2.475</td></tr><tr><td>train/total_flos</td><td>3973611949056.0</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>_runtime</td><td>6</td></tr><tr><td>_timestamp</td><td>1630928195</td></tr><tr><td>_step</td><td>16</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>_runtime</td><td>▁</td></tr><tr><td>_timestamp</td><td>▁</td></tr><tr><td>_step</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">../coba</strong>: <a href=\"https://wandb.ai/kata-research/huggingface/runs/1tj744hf\" target=\"_blank\">https://wandb.ai/kata-research/huggingface/runs/1tj744hf</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_builder.create(\"../untitled.txt\", \"../coba\", hf_trainer_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep  6 11:27:01 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 3090    Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 32%   61C    P2   124W / 350W |   2051MiB / 24265MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 3090    Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "| 96%   74C    P2   308W / 350W |  24262MiB / 24268MiB |     88%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1450922      C   ...ared_envs/atom/bin/python     2049MiB |\n",
      "|    1   N/A  N/A   1431664      C   ...s/tf-torch-tts/bin/python    23617MiB |\n",
      "|    1   N/A  N/A   1450922      C   ...ared_envs/atom/bin/python      639MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!echo $CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f666e5f37850a809725532b3b676272b45d81536514f49468aa4e63f494488bf"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
